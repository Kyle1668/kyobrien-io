<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Kyle O'Brien - AI Safety Researcher | Capability Prevention & Removal</title>

    <!-- SEO Meta Description -->
    <meta name="description" content="AI Safety Research Fellow at ERA Cambridge. Developing techniques for removing dangerous capabilities from AI models through pretraining interventions, machine unlearning, and interpretability research.">
    <meta name="keywords" content="Kyle O'Brien, AI safety, capability prevention, machine unlearning, pretraining safety, biorisk prevention, ERA Cambridge, EleutherAI, deep ignorance, interpretability, sparse autoencoders, LLM safety, AGI alignment, technical AI governance">
    <meta name="author" content="Kyle O'Brien">
    <link rel="canonical" href="https://www.kyobrien.io">
    
    <!-- Additional SEO Tags -->
    <meta name="robots" content="index, follow">
    <meta name="language" content="English">
    <meta name="revisit-after" content="7 days">

    <!-- Open Graph Meta Tags -->
    <meta property="og:title" content="Kyle O'Brien - AI Safety Researcher">
    <meta property="og:description" content="ERA Cambridge Research Fellow developing scalable techniques for removing dangerous capabilities from AI models before deployment. Author of Deep Ignorance and research on pretraining safety interventions.">
    <meta property="og:image" content="https://www.kyobrien.io/images/profile.jpg">
    <meta property="og:url" content="https://www.kyobrien.io">
    <meta property="og:type" content="profile">
    <meta property="og:site_name" content="Kyle O'Brien - AI Safety Research">
    <meta property="og:locale" content="en_US">

    <!-- Twitter Card Meta Tags -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Kyle O'Brien - AI Safety Researcher">
    <meta name="twitter:description" content="ERA Cambridge Fellow developing techniques for removing dangerous AI capabilities. Research on pretraining safety, machine unlearning & interpretability.">
    <meta name="twitter:image" content="https://www.kyobrien.io/images/profile.jpg">
    <meta name="twitter:site" content="@kyletokens">
    <meta name="twitter:creator" content="@kyletokens">

    <link rel="stylesheet" href="styles.css">
    <link rel="icon" type="image/png" href="images/favicon.png">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    
    <!-- Structured Data for SEO -->
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "Person",
        "name": "Kyle O'Brien",
        "jobTitle": "AI Safety Research Fellow",
        "affiliation": {
            "@type": "Organization",
            "name": "ERA Cambridge"
        },
        "alumniOf": [
            {
                "@type": "Organization",
                "name": "EleutherAI"
            },
            {
                "@type": "Organization",
                "name": "Microsoft"
            }
        ],
        "url": "https://www.kyobrien.io",
        "image": "https://www.kyobrien.io/images/profile.jpg",
        "sameAs": [
            "https://github.com/kyle1668",
            "https://www.linkedin.com/in/kyle1668/",
            "https://scholar.google.com/citations?user=YOUR_ID",
            "https://bsky.app/profile/kyletokens.bsky.social",
            "https://substack.com/@kyletokens"
        ],
        "description": "AI Safety Research Fellow developing scalable techniques for removing dangerous capabilities from AI models through pretraining interventions and machine unlearning.",
        "knowsAbout": [
            "AI Safety",
            "Machine Learning",
            "Capability Prevention",
            "Machine Unlearning",
            "Interpretability",
            "Large Language Models",
            "Pretraining Safety",
            "Technical AI Governance"
        ],
        "email": "kyledevinobrien1@gmail.com"
    }
    </script>
</head>
<body>
    <div class="container">
        <header>
            <img src="images/profile.jpg" alt="Kyle O'Brien" class="profile-image">
            <h1>Kyle O'Brien</h1>
            <p class="subtitle">Trying to understand the minds on our computers</p>

            <nav>
                <a href="https://scholar.google.com/citations?user=YOUR_ID" class="nav-link" target="_blank">
                    <svg viewBox="0 0 24 24" class="social-icon">
                        <path fill="currentColor" d="M5.242 13.769L0 9.5 12 0l12 9.5-5.242 4.269C17.548 11.249 14.978 9.5 12 9.5c-2.977 0-5.548 1.748-6.758 4.269zM12 10a7 7 0 1 0 0 14 7 7 0 0 0 0-14z"/>
                    </svg>
                    <span>Scholar</span>
                </a>
                <span class="nav-separator">|</span>
                <a href="https://substack.com/@kyletokens" class="nav-link" target="_blank">
                    <svg viewBox="0 0 24 24" class="social-icon">
                        <path fill="currentColor" d="M22.539 8.242H1.46V5.406h21.08v2.836zM1.46 10.812V24L12 18.11 22.54 24V10.812H1.46zM22.54 0H1.46v2.836h21.08V0z"/>
                    </svg>
                    <span>Substack</span>
                </a>
                <span class="nav-separator">|</span>
                <a href="https://www.linkedin.com/in/kyle1668/" class="nav-link" target="_blank">
                    <svg viewBox="0 0 24 24" class="social-icon">
                        <path fill="currentColor" d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/>
                    </svg>
                    <span>LinkedIn</span>
                </a>
                <span class="nav-separator">|</span>
                <a href="https://bsky.app/profile/kyletokens.bsky.social" class="nav-link" target="_blank">
                    <svg viewBox="0 0 24 24" class="social-icon">
                        <path fill="currentColor" d="M12 10.8c-1.087-2.114-4.046-6.053-6.798-7.995C2.566.944 1.561 1.266.902 1.565.139 1.908 0 3.08 0 3.768c0 .69.378 5.65.624 6.479.815 2.736 3.713 3.66 6.383 3.364.136-.02.275-.039.415-.056-.138.022-.276.04-.415.056-3.912.58-7.387 2.005-2.83 7.078 5.013 5.19 6.87-1.113 7.823-4.308.953 3.195 2.05 9.271 7.733 4.308 4.267-4.308 1.172-6.498-2.74-7.078a8.741 8.741 0 0 1-.415-.056c.14.017.279.036.415.056 2.67.297 5.568-.628 6.383-3.364.246-.828.624-5.79.624-6.478 0-.69-.139-1.861-.902-2.206-.659-.298-1.664-.62-4.3 1.24C16.046 4.748 13.087 8.687 12 10.8Z"/>
                    </svg>
                    <span>Bluesky</span>
                </a>
                <span class="nav-separator">|</span>
                <a href="https://github.com/kyle1668" class="nav-link" target="_blank">
                    <svg viewBox="0 0 98 96" xmlns="http://www.w3.org/2000/svg" class="social-icon">
                        <path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z" fill="currentColor"/>
                    </svg>
                    <span>GitHub</span>
                </a>
            </nav>
        </header>

        <main>
            <section id="current">
                <h2>Current Work</h2>
                <div class="position-card">
                    <p>I'm an AI Safety Research Fellow with <a href="https://erafellowship.org/fellowship" target="_blank"><strong>ERA Cambridge</strong></a>, where I'm studying pretraining and midtraining safety interventions for addressing dangerous capabilities (e.g., biorisk and offensive cyber). I was previously at <a href="https://www.eleuther.ai/" target="_blank"><strong>EleutherAI</strong></a>. I was an applied scientist and software engineer at <a href="https://www.microsoft.com/" target="_blank"><strong>Microsoft</strong></a> before pivoting into AI research.</p>
                    <p>I'm actively seeking research opportunities starting in Fall 2025.</p>
                    <p>For details on my past work and experience, please see my <a href="kyle_obrien_resume.pdf" target="_blank"><strong>resume</strong></a>.</p>
                </div>
            </section>

            <section id="direction">
                <h2>Research Direction</h2>
                <p>
                    My north star is to help make AGI go well for humanity. There are many ways it may not. I don't study them all. My research focuses on developing solutions to the technical and governance challenges. I focus on LLMs because I believe they will lead to AGI and beyond.
                </p>
                <p>
                    My current primary research direction is <strong>Capability Prevention & Removal (CPR)</strong>. This multi-paper direction aims to build scalable techniques for removing unwanted capabilities from models before deployment. Crucially, it shouldn't be easy for downstream users to fine-tune models on these capabilities. For instance, we want to be able to prevent models from learning biorisk capabilities in the first place and develop unlearning algorithms to remove any remaining biorisk knowledge from the weights of a fully-trained model. The ability for model providers to control which capabilities their models have would be a significant breakthrough for AI safety. However, this direction may not scale to ASI, and it is best complemented, and hopefully replaced, by breakthroughs in foundational AI alignment.
                </p>
                <p>
                    We recently released our first paper in this direction — <a href="https://deepignorance.ai/" target="_blank"><strong>Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs</strong></a>. We find that we can prevent LLMs from learning about biorisk by filtering pretraining data, and that this is an especially promising intervention for open-weight models. This work has been covered for a general audience by the <a href="https://www.washingtonpost.com/newsletter/politics/2025/08/12/ai-systems-ignorant-sensitive-data-can-be-safer-still-smart/" target="_blank"><strong>Washington Post</strong></a> and <a href="https://fortune.com/2025/08/14/worried-ai-could-teach-people-to-build-bioweapons-dont-teach-it-how-say-researchers/" target="_blank"><strong>Fortune</strong></a>.
                </p>
                <p>
                    My secondary interests focus on empirically studying <a href="https://turntrout.com/self-fulfilling-misalignment" target="_blank">self-fulfilling misalignment</a>, safety evaluation data contamination, training open-weight models, and technical AI governance.
                </p>
            </section>

            <section id="publications">
                <h2>Selected Publications</h2>
                <div class="publications-list">
                    <div class="publication-item">
                        <h3><a href="https://arxiv.org/abs/2508.06601" target="_blank">Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs</a></h3>
                        <p class="authors">Kyle O'Brien, Stephen Casper, Quentin Anthony, Tomek Korbak, Robert Kirk, Xander Davies, Ishan Mishra, Geoffrey Irving, Yarin Gal, Stella Biderman</p>
                        <p class="venue">arXiv preprint, 2025</p>
                        <img src="images/deep_ignorance_fig.png" alt="Deep Ignorance figure" class="publication-figure">
                        <blockquote class="abstract">We investigate whether filtering dual-use topics from training data can serve as a tamper-resistant safeguard for open-weight LLMs. Our multi-stage data filtering pipeline demonstrates substantial resistance to adversarial fine-tuning on up to 10,000 steps and 300M tokens of biothreat-related text, outperforming existing post-training baselines by over an order of magnitude, with no degradation to unrelated capabilities.</blockquote>
                    </div>

                    <div class="publication-item">
                        <h3><a href="https://arxiv.org/abs/2411.11296" target="_blank">Steering Language Model Refusal with Sparse Autoencoders</a></h3>
                        <p class="authors">Kyle O'Brien, D. Majercak, Xavier Fernandes, Richard Edgar, Jingya Chen, Harsha Nori, Dean Carignan, Eric Horvitz, Forough Poursabzi-Sangdeh</p>
                        <p class="venue">ICML 2025 Workshop on Actionable Interpretability</p>
                        <img src="images/steering_sae_fig.png" alt="Steering SAE figure" class="publication-figure">
                        <blockquote class="abstract">We explore steering model activations at inference time via amplifying sparse autoencoder (SAE) features that mediate refusal. While feature steering successfully improves robustness against jailbreak attempts, we discover a fundamental tension between SAE steering-based safety improvements and general model capabilities, with systematic degradation of performance across benchmark tasks.</blockquote>
                    </div>

                    <div class="publication-item">
                        <h3><a href="https://arxiv.org/abs/2407.06483" target="_blank">Composable Interventions for Language Models</a></h3>
                        <p class="authors">Arinbjörn Kolbeinsson, Kyle O'Brien, Tianjin Huang, Shanghua Gao, Shiwei Liu, Jonathan Richard Schwarz, Anurag Vaidya, Faisal Mahmood, M. Zitnik, Tianlong Chen, Thomas Hartvigsen</p>
                        <p class="venue">International Conference on Learning Representations (ICLR), 2024</p>
                        <img src="images/composable_fig.png" alt="Composable Interventions figure" class="publication-figure">
                        <blockquote class="abstract">We introduce composable interventions, a framework to study the effects of using multiple interventions on the same language models. Using our framework, we conduct extensive experiments composing popular methods from Knowledge Editing, Model Compression, and Machine Unlearning, revealing complex interaction patterns when interventions are applied sequentially.</blockquote>
                    </div>

                    <div class="publication-item">
                        <h3><a href="https://arxiv.org/abs/2406.17746" target="_blank">Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted Phenomenon</a></h3>
                        <p class="authors">USVSN Sai Prashanth, Alvin Deng, Kyle O'Brien, V Jyothir S, Mohammad Aflah Khan, Jaydeep Borkar, Christopher A. Choquette-Choo, Jacob Ray Fuehne, Stella Biderman, Tracy Ke, Katherine Lee, Naomi Saphra</p>
                        <p class="venue">International Conference on Learning Representations (ICLR), 2024</p>
                        <img src="images/recite_fig.png" alt="Recite, Reconstruct, Recollect figure" class="publication-figure">
                        <blockquote class="abstract">We model memorization as a multifaceted phenomenon, introducing a taxonomy that breaks it into recitation (of highly duplicated sequences), reconstruction (of inherently predictable sequences), and recollection (of sequences that are neither). We demonstrate the taxonomy's usefulness by constructing a predictive model showing different factors influence memorization likelihood across categories.</blockquote>
                    </div>

                    <div class="publication-item">
                        <h3><a href="https://arxiv.org/abs/2402.08225" target="_blank">Improving Black-box Robustness with In-Context Rewriting</a></h3>
                        <p class="authors">Kyle O'Brien, Nathan Ng, Isha Puri, Jorge Mendez, Hamid Palangi, Yoon Kim, Marzyeh Ghassemi, Tom Hartvigsen</p>
                        <p class="venue">Transactions on Machine Learning Research (TMLR), 2024</p>
                        <img src="images/icr_fig.png" alt="In-Context Rewriting figure" class="publication-figure">
                        <blockquote class="abstract">We propose LLM-TTA, which uses LLM-generated augmentations for test-time augmentation to improve model robustness on out-of-distribution inputs. Our In-Context Rewriting method rewrites inputs to match in-distribution exemplars, outperforming conventional augmentation functions for BERT and T5 across sentiment, toxicity, and news classification tasks.</blockquote>
                    </div>

                    <div class="publication-item">
                        <h3><a href="https://arxiv.org/abs/2304.01373" target="_blank">Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling</a></h3>
                        <p class="authors">Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, Oskar van der Wal</p>
                        <p class="venue">International Conference on Machine Learning (ICML), 2023</p>
                        <img src="images/pythia_fig.png" alt="Pythia figure" class="publication-figure">
                        <blockquote class="abstract">We introduce Pythia, a suite of 16 LLMs ranging from 70M to 12B parameters, all trained on public data in the exact same order. We provide 154 checkpoints per model alongside tools to reconstruct training dataloaders, facilitating research in memorization, few-shot performance, and bias reduction through this highly controlled setup.</blockquote>
                    </div>
                </div>
            </section>

            <section id="contact">
                <h2>Get in Touch</h2>
                <div class="contact-box">
                    <p>
                        I'm generally happy to meet people interested in my research, potential collaborations, or AI safety career advice.
                    </p>
                    <p class="email">
                        <strong>Email:</strong> <a href="mailto:kyledevinobrien1@gmail.com">kyledevinobrien1@gmail.com</a>
                    </p>
                </div>
            </section>
        </main>

        <footer>
            <div class="footer-links">
                <a href="https://scholar.google.com/citations?user=YOUR_ID" target="_blank" aria-label="Google Scholar">
                    <svg viewBox="0 0 24 24" class="footer-icon">
                        <path fill="currentColor" d="M5.242 13.769L0 9.5 12 0l12 9.5-5.242 4.269C17.548 11.249 14.978 9.5 12 9.5c-2.977 0-5.548 1.748-6.758 4.269zM12 10a7 7 0 1 0 0 14 7 7 0 0 0 0-14z"/>
                    </svg>
                </a>
                <a href="https://substack.com/@kyletokens" target="_blank" aria-label="Substack">
                    <svg viewBox="0 0 24 24" class="footer-icon">
                        <path fill="currentColor" d="M22.539 8.242H1.46V5.406h21.08v2.836zM1.46 10.812V24L12 18.11 22.54 24V10.812H1.46zM22.54 0H1.46v2.836h21.08V0z"/>
                    </svg>
                </a>
                <a href="https://www.linkedin.com/in/kyle1668/" target="_blank" aria-label="LinkedIn">
                    <svg viewBox="0 0 24 24" class="footer-icon">
                        <path fill="currentColor" d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/>
                    </svg>
                </a>
                <a href="https://bsky.app/profile/kyletokens.bsky.social" target="_blank" aria-label="Bluesky">
                    <svg viewBox="0 0 24 24" class="footer-icon">
                        <path fill="currentColor" d="M12 10.8c-1.087-2.114-4.046-6.053-6.798-7.995C2.566.944 1.561 1.266.902 1.565.139 1.908 0 3.08 0 3.768c0 .69.378 5.65.624 6.479.815 2.736 3.713 3.66 6.383 3.364.136-.02.275-.039.415-.056-.138.022-.276.04-.415.056-3.912.58-7.387 2.005-2.83 7.078 5.013 5.19 6.87-1.113 7.823-4.308.953 3.195 2.05 9.271 7.733 4.308 4.267-4.308 1.172-6.498-2.74-7.078a8.741 8.741 0 0 1-.415-.056c.14.017.279.036.415.056 2.67.297 5.568-.628 6.383-3.364.246-.828.624-5.79.624-6.478 0-.69-.139-1.861-.902-2.206-.659-.298-1.664-.62-4.3 1.24C16.046 4.748 13.087 8.687 12 10.8Z"/>
                    </svg>
                </a>
                <a href="https://github.com/kyle1668" target="_blank" aria-label="GitHub">
                    <svg viewBox="0 0 98 96" xmlns="http://www.w3.org/2000/svg" class="footer-icon">
                        <path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z" fill="currentColor"/>
                    </svg>
                </a>
            </div>
        </footer>
    </div>
</body>
</html>